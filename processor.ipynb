{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GipsyX processor\n",
    "### 1. Conversion, analysis and merging  of rinex files into 30 h arc files\n",
    "### 2. TropNominals generation with VMF1 model\n",
    "### 3. IONEX merging\n",
    "### 4. Tree files generation (for each year and IONEX file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:56:54.038057Z",
     "start_time": "2018-10-08T06:56:53.809736Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import rmtree\n",
    "from multiprocessing import Pool\n",
    "import os, sys, re, subprocess, calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:56:54.527538Z",
     "start_time": "2018-10-08T06:56:54.399649Z"
    },
    "code_folding": [
     1,
     54,
     110
    ]
   },
   "outputs": [],
   "source": [
    "#All on options\n",
    "tree_options = [\n",
    "                    # Stochastic Adjustment State\n",
    "                    ['GRN_STATION_CLK_WHITE:State:Pos:StochasticAdj', '10 10 $GLOBAL_DATA_RATE WHITENOISE'],\n",
    "                    # Tides section\n",
    "                    ['GRN_STATION_CLK_WHITE:Tides:All', 'On'],\n",
    "#                     ['GRN_STATION_CLK_WHITE:Tides:OceanLoad', '$OCEANLOAD'], #not needed as \"all on\"\n",
    "                    ['GRN_STATION_CLK_WHITE:Tides:OceanLoadFile', '/mnt/Data/bogdanm/tmp_GipsyX/otl/ocnld_coeff/bigf.blq'],\n",
    "                    # Trop section    \n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradEast', '0.0'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradEast:StochasticAdj', '1.0 5e-6 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradNorth', '0.0'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradNorth:StochasticAdj', '1.0 5e-6 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:Mapping', 'VMF1'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:Model', 'On'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:WetZ', '0.1'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:WetZ:StochasticAdj', '0.5 5e-5 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    # Station properties. e.g. CAMB. So we will get wetz etc in the output. Highly important!!!\n",
    "                    ['Station', '`cat $STATIONLIST`'+'\\nStation `staDb2TreeIn.py -s $STATIONLIST -y2kSecs $GLOBAL_EPOCH -d $STA_DB`'],\n",
    "                    # VMF1dataDir path  \n",
    "                    ['Station:VMF1dataDir', '/mnt/Data/bogdanm/Products/VMF1_Products'],\n",
    "                    # Ion2nd\n",
    "                    ['Global:Ion2nd','On'],\n",
    "                    ['Global:Ion2nd:MagneticModel','IGRF'],\n",
    "                    ['Global:Ion2nd:MagneticModel:IgrfCoefficientsFile', '$GOA_VAR/etc/igrf/igrf11coeffs.txt'],\n",
    "                    ['Global:Ion2nd:StecModel', 'IONEX'],\n",
    "                    ['Global:Ion2nd:StecModel:IonexFile:ShellHeight', '600.0e3'],\n",
    "                    # GPS_BlockII as it is missing from default tree file\n",
    "                    ['GPS_BlockII_Model', '=='],\n",
    "                    ['GPS_BlockII_Model:AttitudeModel', 'gpsBlockII'],\n",
    "                    # Satellite section needed for Gipsy to be able to extract satellite properties. Highly important!!!\n",
    "                    ['Satellite','`cat $GNSSLIST`'+ '\\nSatellite `pcm.py -file $GNSS_ANT_OFF_PCM -epoch $GLOBAL_EPOCH -sat $GNSSLIST -param Antenna1`'],\n",
    "   \n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Degree','0'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:MaxFormalError','0.4'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Strict','On'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Strict:MaxDx','1.0e-6'],\n",
    "                    # Section with namefilter for state\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Station\\..*\\.State\\.Pos\\..*',' '],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Station\\..*\\.Trop.*',' '],\n",
    "    \n",
    "\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreePhaseC1C5:DataLinkSpec_LC_BDS:PostSmoothEdit','2e5 2e4 0.25 0.2 0.1 .05'],    \n",
    "                    ['Global:DataTypes:IonoFreePhaseC1C5:DataLinkSpec_LC_BDS:SignalPath:Platforms','.* C.*'],\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreePhaseP1P2:DataLinkSpec_LC_GPS:PostSmoothEdit','2e5 2e4 0.125 0.1 0.05 .025'],\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreeRangeC1C5:DataLinkSpec_PC_BDS:PostSmoothEdit','2e5 2e4 25 20 10 5'],    \n",
    "                    ['Global:DataTypes:IonoFreeRangeC1C5:DataLinkSpec_PC_BDS:SignalPath:Platforms','.* C.*'],     \n",
    "\n",
    "                    ['Global:DataTypes:IonoFreeRangeP1P2:DataLinkSpec_PC_GPS:PostSmoothEdit','2e5 2e4 12.5 10 5 2.5'],     \n",
    "                ]\n",
    "\n",
    "tree_options_tides_off = [\n",
    "                    # Stochastic Adjustment State\n",
    "                    ['GRN_STATION_CLK_WHITE:State:Pos:StochasticAdj', '10 10 $GLOBAL_DATA_RATE WHITENOISE'],\n",
    "                    # Tides section\n",
    "                    ['GRN_STATION_CLK_WHITE:Tides:All', 'Off'],\n",
    "#                     ['GRN_STATION_CLK_WHITE:Tides:OceanLoad', '$OCEANLOAD'], #not needed as \"all on\"\n",
    "                    ['GRN_STATION_CLK_WHITE:Tides:OceanLoadFile', '/mnt/Data/bogdanm/tmp_GipsyX/otl/ocnld_coeff/bigf.blq'],\n",
    "                    # Trop section    \n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradEast', '0.0'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradEast:StochasticAdj', '1.0 5e-6 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradNorth', '0.0'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradNorth:StochasticAdj', '1.0 5e-6 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:Mapping', 'VMF1'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:Model', 'On'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:WetZ', '0.1'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:WetZ:StochasticAdj', '0.5 5e-5 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    # Station properties. e.g. CAMB. So we will get wetz etc in the output. Highly important!!!\n",
    "                    ['Station', '`cat $STATIONLIST`'+'\\nStation `staDb2TreeIn.py -s $STATIONLIST -y2kSecs $GLOBAL_EPOCH -d $STA_DB`'],\n",
    "                    # VMF1dataDir path  \n",
    "                    ['Station:VMF1dataDir', '/mnt/Data/bogdanm/Products/VMF1_Products'],\n",
    "                    # Ion2nd\n",
    "                    ['Global:Ion2nd','On'],\n",
    "                    ['Global:Ion2nd:MagneticModel','IGRF'],\n",
    "                    ['Global:Ion2nd:MagneticModel:IgrfCoefficientsFile', '$GOA_VAR/etc/igrf/igrf11coeffs.txt'],\n",
    "                    ['Global:Ion2nd:StecModel', 'IONEX'],\n",
    "                    ['Global:Ion2nd:StecModel:IonexFile:ShellHeight', '600.0e3'],\n",
    "                    # GPS_BlockII as it is missing from default tree file\n",
    "                    ['GPS_BlockII_Model', '=='],\n",
    "                    ['GPS_BlockII_Model:AttitudeModel', 'gpsBlockII'],\n",
    "                    # Satellite section needed for Gipsy to be able to extract satellite properties. Highly important!!!\n",
    "                    ['Satellite','`cat $GNSSLIST`'+ '\\nSatellite `pcm.py -file $GNSS_ANT_OFF_PCM -epoch $GLOBAL_EPOCH -sat $GNSSLIST -param Antenna1`'],\n",
    "   \n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Degree','0'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:MaxFormalError','0.4'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Strict','On'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Strict:MaxDx','1.0e-6'],\n",
    "                    # Section with namefilter for state\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Station\\..*\\.State\\.Pos\\..*',' '],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Station\\..*\\.Trop.*',' '],\n",
    "    \n",
    "\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreePhaseC1C5:DataLinkSpec_LC_BDS:PostSmoothEdit','2e5 2e4 0.25 0.2 0.1 .05'],    \n",
    "                    ['Global:DataTypes:IonoFreePhaseC1C5:DataLinkSpec_LC_BDS:SignalPath:Platforms','.* C.*'],\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreePhaseP1P2:DataLinkSpec_LC_GPS:PostSmoothEdit','2e5 2e4 0.125 0.1 0.05 .025'],\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreeRangeC1C5:DataLinkSpec_PC_BDS:PostSmoothEdit','2e5 2e4 25 20 10 5'],    \n",
    "                    ['Global:DataTypes:IonoFreeRangeC1C5:DataLinkSpec_PC_BDS:SignalPath:Platforms','.* C.*'],     \n",
    "\n",
    "                    ['Global:DataTypes:IonoFreeRangeP1P2:DataLinkSpec_PC_GPS:PostSmoothEdit','2e5 2e4 12.5 10 5 2.5'],     \n",
    "                ]\n",
    "\n",
    "#Might need to create tree options with no otl correction enabled\n",
    "\n",
    "'''tree with ambres off'''\n",
    "tree_options_ambres_off = [\n",
    "                    # Stochastic Adjustment State\n",
    "                    ['GRN_STATION_CLK_WHITE:State:Pos:StochasticAdj', '10 10 $GLOBAL_DATA_RATE WHITENOISE'],\n",
    "                    # Tides section\n",
    "                    ['GRN_STATION_CLK_WHITE:Tides:All', 'On'],\n",
    "#                     ['GRN_STATION_CLK_WHITE:Tides:OceanLoad', '$OCEANLOAD'], #not needed as \"all on\"\n",
    "                    ['GRN_STATION_CLK_WHITE:Tides:OceanLoadFile', '/mnt/Data/bogdanm/tmp_GipsyX/otl/ocnld_coeff/bigf.blq'],\n",
    "                    # Trop section    \n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradEast', '0.0'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradEast:StochasticAdj', '1.0 5e-6 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradNorth', '0.0'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:GradNorth:StochasticAdj', '1.0 5e-6 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:Mapping', 'VMF1'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:Model', 'On'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:WetZ', '0.1'],\n",
    "                    ['GRN_STATION_CLK_WHITE:Trop:WetZ:StochasticAdj', '0.5 5e-5 $GLOBAL_DATA_RATE RANDOMWALK'],\n",
    "                    # Station properties. e.g. CAMB. So we will get wetz etc in the output. Highly important!!!\n",
    "                    ['Station', '`cat $STATIONLIST`'+'\\nStation `staDb2TreeIn.py -s $STATIONLIST -y2kSecs $GLOBAL_EPOCH -d $STA_DB`'],\n",
    "                    # VMF1dataDir path  \n",
    "                    ['Station:VMF1dataDir', '/mnt/Data/bogdanm/Products/VMF1_Products'],\n",
    "                    # Ion2nd\n",
    "                    ['Global:Ion2nd','On'],\n",
    "                    ['Global:Ion2nd:MagneticModel','IGRF'],\n",
    "                    ['Global:Ion2nd:MagneticModel:IgrfCoefficientsFile', '$GOA_VAR/etc/igrf/igrf11coeffs.txt'],\n",
    "                    ['Global:Ion2nd:StecModel', 'IONEX'],\n",
    "                    ['Global:Ion2nd:StecModel:IonexFile:ShellHeight', '600.0e3'],\n",
    "                    # GPS_BlockII as it is missing from default tree file\n",
    "                    ['GPS_BlockII_Model', '=='],\n",
    "                    ['GPS_BlockII_Model:AttitudeModel', 'gpsBlockII'],\n",
    "                    # Satellite section needed for Gipsy to be able to extract satellite properties. Highly important!!!\n",
    "                    ['Satellite','`cat $GNSSLIST`'+ '\\nSatellite `pcm.py -file $GNSS_ANT_OFF_PCM -epoch $GLOBAL_EPOCH -sat $GNSSLIST -param Antenna1`'],\n",
    "                    #AmbRes to off\n",
    "                    ['Global:AmbRes','Off'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Degree','0'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:MaxFormalError','0.4'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Strict','On'],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Satellite\\.[C]\\w*.*\\.Clk\\.Bias:Strict:MaxDx','1.0e-6'],\n",
    "                    # Section with namefilter for state\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Station\\..*\\.State\\.Pos\\..*',' '],\n",
    "                    ['Global:Input:TimeDepParms:NameFilter:Station\\..*\\.Trop.*',' '],\n",
    "\n",
    "                    ['Global:DataTypes:IonoFreePhaseC1C5:DataLinkSpec_LC_BDS:PostSmoothEdit','2e5 2e4 0.25 0.2 0.1 .05'],    \n",
    "                    ['Global:DataTypes:IonoFreePhaseC1C5:DataLinkSpec_LC_BDS:SignalPath:Platforms','.* C.*'],\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreePhaseP1P2:DataLinkSpec_LC_GPS:PostSmoothEdit','2e5 2e4 0.125 0.1 0.05 .025'],\n",
    "    \n",
    "                    ['Global:DataTypes:IonoFreeRangeC1C5:DataLinkSpec_PC_BDS:PostSmoothEdit','2e5 2e4 25 20 10 5'],    \n",
    "                    ['Global:DataTypes:IonoFreeRangeC1C5:DataLinkSpec_PC_BDS:SignalPath:Platforms','.* C.*'],     \n",
    "\n",
    "                    ['Global:DataTypes:IonoFreeRangeP1P2:DataLinkSpec_PC_GPS:PostSmoothEdit','2e5 2e4 12.5 10 5 2.5'],     \n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:56:55.369302Z",
     "start_time": "2018-10-08T06:56:55.361049Z"
    }
   },
   "outputs": [],
   "source": [
    "from gxlib import gx_aux,gx_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:57:02.492967Z",
     "start_time": "2018-10-08T06:56:55.978204Z"
    },
    "code_folding": [
     56,
     63,
     71,
     83,
     95,
     118,
     203,
     221,
     238,
     253,
     296,
     326,
     365,
     423
    ]
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shutil import rmtree\n",
    "from multiprocessing import Pool\n",
    "import os, sys, re, subprocess, calendar\n",
    "\n",
    "PYGCOREPATH=\"{}/lib/python{}.{}\".format(os.environ['GCOREBUILD'],\n",
    "                            sys.version_info[0], sys.version_info[1])\n",
    "if PYGCOREPATH not in sys.path:\n",
    "    sys.path.insert(0,PYGCOREPATH)\n",
    "\n",
    "import gipsyx.tropNom as tropNom\n",
    "import gcore.StationDataBase as StationDataBase\n",
    "import gcore.treeUtils as treeUtils\n",
    "\n",
    "\n",
    "class rnx2dr:\n",
    "    def __init__(self,\n",
    "                 project_name,\n",
    "                 stations_list,\n",
    "                 years_list,\n",
    "                 rnx_dir='/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s',\n",
    "                 tmp_dir='/mnt/Data/bogdanm/tmp_GipsyX',\n",
    "                 VMF1_dir = '/mnt/Data/bogdanm/Products/VMF1_Products',\n",
    "                 tropNom_type = '30h_tropNominalOut_VMF1.tdp',\n",
    "                 IGS_logs_dir = '/mnt/Data/bogdanm/GNSS_data/BIGF_data/station_log_files',\n",
    "                 tree_options=tree_options,\n",
    "                 rate = 300,\n",
    "                 gnss_products_dir = '/mnt/Data/bogdanm/Products/JPL_GPS_Products/Final',\n",
    "                 ionex_type='igs', #No ionex dir required as ionex merged products will be put into tmp directory by ionex class\n",
    "                 num_cores = 8):\n",
    "        \n",
    "        self.project_name = project_name\n",
    "        self.IGS_logs_dir = IGS_logs_dir\n",
    "        self.rnx_dir=rnx_dir\n",
    "        self.tmp_dir=tmp_dir\n",
    "        self.stations_list=stations_list\n",
    "        self.years_list=years_list\n",
    "        self.num_cores = num_cores\n",
    "        self.J2000origin = np.datetime64('2000-01-01 12:00:00')\n",
    "        self.VMF1_dir = VMF1_dir\n",
    "        self.tropNom_type = tropNom_type\n",
    "        self.tree_options = tree_options\n",
    "        self.rnx_files = self._select_rnx()\n",
    "        self.staDb_dir= gx_aux.gen_staDb(self.tmp_dir,self.project_name,self.stations_list,self.IGS_logs_dir)\n",
    "        self.gnss_products_dir = gnss_products_dir\n",
    "        self.ionex_type=ionex_type\n",
    "        self.rate=rate\n",
    "        \n",
    "    def _select_rnx(self):\n",
    "        return gx_aux.select_rnx(rnx_dir=self.rnx_dir,stations_list=self.stations_list,years_list=self.years_list)\n",
    "    def analyse(self):\n",
    "        return gx_aux.analyse(rnx_files=self.rnx_files,stations_list=self.stations_list,years_list=self.years_list)\n",
    "    def rnx2dr(self):\n",
    "        gx_convert.rnx2dr(rnx_files=self.rnx_files, stations_list=self.stations_list, tmp_dir=self.tmp_dir, num_cores=self.num_cores)\n",
    "\n",
    "    def _dr_size(self,rnx_files):\n",
    "        size_array= np.ndarray((rnx_files.shape),dtype = object)\n",
    "        bad_files = np.ndarray((rnx_files.shape),dtype = object)\n",
    "        good_files = np.ndarray((rnx_files.shape),dtype = object)        \n",
    "\n",
    "        for i in range(len(rnx_files)):\n",
    "            tmp = np.ndarray((len(rnx_files[i])))\n",
    "            for j in range(len(tmp)):\n",
    "                tmp[j] = os.path.getsize(rnx_files[i][j,1]) #index of 1 means dr file path\n",
    "\n",
    "            size_array[i] = np.column_stack((rnx_files[i],tmp))\n",
    "            bad_files[i] = rnx_files[i][tmp==20]\n",
    "            good_files[i] = rnx_files[i][tmp!=20]\n",
    "\n",
    "        return size_array,bad_files,good_files        \n",
    "    def _drinfo(self, file):\n",
    "        drInfo_process = subprocess.Popen(args=['dataRecordInfo', '-file', os.path.basename(file)],\n",
    "                                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=os.path.dirname(file))\n",
    "        out, err = drInfo_process.communicate()\n",
    "        dr_Info_raw = pd.Series(out.decode(\n",
    "            'ascii').splitlines()).str.split(pat=':\\s', expand=True)\n",
    "\n",
    "        number_of_records = pd.to_numeric(dr_Info_raw.iloc[0, 1])\n",
    "        timeframe = pd.to_datetime(dr_Info_raw.iloc[1:3, 1])\n",
    "        number_of_receivers = pd.to_numeric(dr_Info_raw.iloc[3, 1])\n",
    "        number_of_transmitters = pd.to_numeric(dr_Info_raw.iloc[5, 1])\n",
    "        site_name = dr_Info_raw.iloc[4, 0].strip()\n",
    "        transmitter_types = np.unique(\n",
    "            ((dr_Info_raw.iloc[6:, 0]).str.strip()).str[:1].values)\n",
    "\n",
    "        return np.asarray((site_name,\n",
    "                              number_of_records,\n",
    "                              np.datetime64(timeframe[1]),\n",
    "                              np.datetime64(timeframe[2]),\n",
    "                              number_of_receivers,\n",
    "                              number_of_transmitters,\n",
    "                              transmitter_types,\n",
    "                              file\n",
    "                              ))\n",
    "    def get_drinfo(self):\n",
    "        num_cores = int(self.num_cores) #safety precaution if str value is specified\n",
    "        rs = np.ndarray((len(self.stations_list)),dtype=object)\n",
    "        rnx_in_out = self._rnx2dr_gen_paths()\n",
    "        dr_size_array,dr_empty,dr_good = self._dr_size(rnx_in_out)\n",
    "\n",
    "        \n",
    "        for i in range(len(self.stations_list)):\n",
    "            num_cores = num_cores if len(dr_good[i]) > num_cores else len(dr_good[i])\n",
    "\n",
    "            chunksize = int(np.ceil(len(dr_good[i]) / num_cores))\n",
    "            print(self.stations_list[i],'station binary files analysis...')\n",
    "            print ('Number of files to process:', len(dr_good[i]),'| Adj. num_cores:', num_cores,'| Chunksize:', chunksize,end=' ')\n",
    "            \n",
    "            pool = Pool(processes=num_cores)\n",
    "            rs[i] = np.asarray(pool.map(func=self._drinfo, iterable=dr_good[i][:,1], chunksize=chunksize))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            print('| Done!')\n",
    "\n",
    "        \n",
    "        #Saving extracted data for furthe processing\n",
    "        np.savez_compressed(file=self.tmp_dir+'/rnx_dr/drinfo',drinfo=rs,stations_list=self.stations_list,years_list=self.years_list)\n",
    "    def get_merge_table(self):\n",
    "        \n",
    "        drinfo_file = np.load(file=self.tmp_dir+'/rnx_dr/drinfo.npz')\n",
    "        drinfo = drinfo_file['drinfo']\n",
    "        \n",
    "        #function for gps records. Othervise no-gps files won't be processed and will always be present in gd2e output\n",
    "        def hasGPS(arr):\n",
    "            return np.max(np.isin(arr,'G')) \n",
    "        \n",
    "\n",
    "        '''Creating classes according to record length'''\n",
    "        dr_classes = np.ndarray((len(drinfo)),dtype=object)\n",
    "        for i in range(len(drinfo)):\n",
    "            \n",
    "            gps_transmitter = np.ndarray((len(drinfo[i])),dtype=bool)\n",
    "            for j in range(len(drinfo[i])):\n",
    "                gps_transmitter[j] = hasGPS(drinfo[i][j,-2])\n",
    "            gps_drinfo = drinfo[i][gps_transmitter]\n",
    "            \n",
    "            \n",
    "            station_record = gps_drinfo #filtered station record\n",
    "            completeness = np.zeros((len(station_record)),dtype=np.int)\n",
    "            drinfo_rec_time = (station_record[:,3].astype('datetime64[h]')-station_record[:,2].astype('datetime64[h]')).astype(np.int)\n",
    "            \n",
    "            #-----------------------------------------------------------------------\n",
    "            # Basic filtering module that uses total length of the datarecord.\n",
    "            # records with more than 12 hours of data but less than 20 hours of data get 1\n",
    "            completeness[((drinfo_rec_time>=12) & ((drinfo_rec_time)<20))]=1\n",
    "\n",
    "            #records with more than 20 hours of data get 2\n",
    "            completeness[(drinfo_rec_time>=20)]=2\n",
    "            #-----------------------------------------------------------------------\n",
    "\n",
    "            # BOUNDARY_1\n",
    "\n",
    "            # start_c - start_p         <=24h & > 2h\n",
    "            #  day      hour\n",
    "\n",
    "            # start_c - end_p           <1h   & >=0h\n",
    "            #  day      hour\n",
    "\n",
    "            # BOUNDARY_2\n",
    "\n",
    "            # end_n   - start_c + 24h   <=48h & > 26h\n",
    "            #  hour     day\n",
    "\n",
    "            # start_n - start_c + 24h   <25h  & >=24h\n",
    "            #  hour     day\n",
    "\n",
    "            #-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            start_c=station_record[:,2]\n",
    "            start_p=np.roll(station_record[:,2],1)\n",
    "            start_n=np.roll(station_record[:,2],-1)\n",
    "\n",
    "            end_c=station_record[:,3]\n",
    "            end_p=np.roll(station_record[:,3],1)\n",
    "            end_n=np.roll(station_record[:,3],-1)\n",
    "\n",
    "            B1c1 = (start_c.astype('datetime64[D]')-start_p.astype('datetime64[h]') <= np.timedelta64(24,'[h]'))\\\n",
    "            &(start_c.astype('datetime64[D]')-start_p.astype('datetime64[h]') > np.timedelta64(2,'[h]'))\n",
    "\n",
    "            B1c2 = (start_c.astype('datetime64[D]')-end_p.astype('datetime64[m]') < np.timedelta64(1,'[h]'))\\\n",
    "            &(start_c.astype('datetime64[D]')-end_p.astype('datetime64[m]') >= np.timedelta64(0,'[m]'))\n",
    "\n",
    "            B2c1 = (end_n.astype('datetime64[h]')-end_c.astype('datetime64[D]') <= np.timedelta64(48,'[h]'))\\\n",
    "            &(end_n.astype('datetime64[h]')-end_c.astype('datetime64[D]') > np.timedelta64(26,'[h]'))\n",
    "\n",
    "            B2c2 = (start_n.astype('datetime64[h]')-end_c.astype('datetime64[D]') < np.timedelta64(25,'[h]'))\\\n",
    "            &(start_n.astype('datetime64[h]')-end_c.astype('datetime64[D]') >= np.timedelta64(24,'[h]'))\n",
    "\n",
    "\n",
    "#             completeness[(B1c1 & B1c2 & B2c1 & B2c2 & completeness==2)] = 3\n",
    "\n",
    "            completeness[B1c1 & B1c2 & B2c1 & B2c2 & (completeness==2)] = 3\n",
    "            dr_classes[i] = np.column_stack((completeness,\n",
    "                                             station_record[:,2], #record start time\n",
    "                                             station_record[:,3], #record end time\n",
    "                                             np.roll(station_record[:,7],1),\n",
    "                                             station_record[:,7],\n",
    "                                             np.roll(station_record[:,7],-1)\n",
    "                                            ))\n",
    "            #     return dataRecordInfo\n",
    "        return dr_classes\n",
    "    def _merge(self, merge_set):\n",
    "        '''Expects a merge set of 3 files [class,time,file0,file1,file2]. Merges all files into file1_30h. file1 must be a class 3 file\n",
    "        Constructs 30h arcs with drMerge.py.\n",
    "        Sample input:\n",
    "        drMerge.py -i isba0940.15o.dr ohln0940.15o.dr -start 2015-04-04 00:00:00 -end 2015-04-04 04:00:00\n",
    "        '''\n",
    "        if not os.path.isfile((merge_set[4])[:-6]+'_30h.dr.gz'):\n",
    "            \n",
    "            drMerge_proc = subprocess.Popen(['drMerge',\\\n",
    "                                     ((merge_set[1].astype('datetime64') - np.timedelta64( 3,'[h]'))-\\\n",
    "                                      self.J2000origin).astype('timedelta64[s]').astype(int).astype(str),\\\n",
    "                                     ((merge_set[1].astype('datetime64') + np.timedelta64(27,'[h]'))-\\\n",
    "                                      self.J2000origin).astype('timedelta64[s]').astype(int).astype(str),\\\n",
    "                                     os.path.basename(merge_set[4])[:-6]+'_30h.dr.gz',\\\n",
    "\n",
    "                                     merge_set[3], merge_set[4], merge_set[5] ],\\\n",
    "                                     cwd=os.path.dirname(merge_set[4]))\n",
    "            drMerge_proc.wait()\n",
    "    def dr_merge(self):\n",
    "        num_cores = int(self.num_cores) #safety precaution if str value is specified\n",
    "        merge_table = self.get_merge_table()\n",
    "\n",
    "        for i in range(len(self.stations_list)):\n",
    "            num_cores = num_cores if len(merge_table[i]) > num_cores else len(merge_table[i])\n",
    "            chunksize = int(np.ceil(len(merge_table[i]) / num_cores))\n",
    "            merge_table_class3 = merge_table[i][merge_table[i][:,0]==3]\n",
    "\n",
    "            print(self.stations_list[i],'station binary files merging...')\n",
    "            print ('Number of files to process:', len(merge_table_class3),'| Adj. num_cores:', num_cores,'| Chunksize:', chunksize,end=' ')\n",
    "            pool = Pool(processes=num_cores)\n",
    "\n",
    "            pool.map_async(func=self._merge, iterable=merge_table_class3, chunksize=chunksize)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            print('| Done!')\n",
    "    def tdpInput(self,tropnom_param):\n",
    "        staDb=StationDataBase.StationDataBase() #creating staDb object\n",
    "        staDb.read(self.staDb_dir) #reading staDb into staDb object\n",
    "        stns = staDb.getStationList() #creating array with available station names\n",
    "\n",
    "        begin,end,tropNom_out = tropnom_param\n",
    "        '''begin, end, station_name, tropNom_out\n",
    "        begin = tropnom begin in J2000 seconds; end = tropnom end in J2000 seconds; station name as in staDb;  '''\n",
    "        if not os.path.isfile(tropNom_out):\n",
    "            if not os.path.exists(os.path.dirname(tropNom_out)):\n",
    "                os.makedirs(os.path.dirname(tropNom_out))\n",
    "\n",
    "            #begin, end, tdp_PATH\n",
    "            nominals=tropNom.nominalTrops('VMF1', modelFile=self.VMF1_dir)\n",
    "            nominals.makeTdp(begin, end, self.rate, stns, tropNom_out, append=False, staDb=staDb, dry=True, wet=True)\n",
    "    def gen_tropnom(self):\n",
    "        '''\n",
    "        Generating tropnominal file for valid stations in staDb file. Cannot make it work with VMF.\n",
    "        file 31 gives error, no matter what year it is. tdp file is created for each observation file\n",
    "        '''\n",
    "        num_cores = int(self.num_cores)\n",
    "        \n",
    "        drinfo_file = np.load(file=self.tmp_dir+'/rnx_dr/drinfo.npz')\n",
    "        drinfo_years_list = drinfo_file['years_list']\n",
    "\n",
    "        #creating folder and file structure taking into account leap year.\n",
    "        #resulting paths look as follows: year/doy/30h_tropNominal.vmf1\n",
    "        #data on next day needed to create current day tropnominal\n",
    "        days_in_year=np.ndarray((len(drinfo_years_list)),dtype=int)\n",
    "        for i in range(len(drinfo_years_list)):\n",
    "            \n",
    "            days_in_year[i] = int(365 + (1*calendar.isleap(drinfo_years_list[i])))\n",
    "            date = (np.datetime64(str(drinfo_years_list[i])) + (np.arange(days_in_year[i]).astype('timedelta64[D]')))-  self.J2000origin\n",
    "            #Now all works correctly. The bug with wrong timevalues was corrected.\n",
    "            begin = (date - np.timedelta64(3,'[h]')).astype(int) \n",
    "            end = (date + np.timedelta64(27,'[h]')).astype(int) \n",
    "\n",
    "            paths = (self.tmp_dir +'/tropNom/'+ str(drinfo_years_list[i])+'/'+pd.Series(np.arange(1,days_in_year[i]+1)).astype(str).str.zfill(3)+'/30h_tropNominalOut_VMF1.tdp').values\n",
    "            tropnom_param = np.column_stack((begin,end,paths))\n",
    "            \n",
    "                \n",
    "            \n",
    "            num_cores = num_cores if len(tropnom_param) > num_cores else len(tropnom_param)\n",
    "            step_size = int(np.ceil(len(tropnom_param) / num_cores))\n",
    "\n",
    "            print(drinfo_years_list[i],'year tropnominals generation...')\n",
    "            print ('Number of files to process:', len(tropnom_param),'| Adj. num_cores:', num_cores,end=' ')\n",
    "\n",
    "            for i in range(step_size):\n",
    "                try:\n",
    "                    pool = Pool(num_cores)\n",
    "                    pool.map(self.tdpInput, tropnom_param[np.arange(i, len(tropnom_param), step_size)])\n",
    "                finally:\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "            print('| Done!')\n",
    "#         return tropnom_param\n",
    "            \n",
    "    def gen_trees(self):\n",
    "        # reading ionex filenames\n",
    "        out_df = pd.DataFrame()\n",
    "#         default_tree = '/home/bogdanm/Desktop/GipsyX_trees/Trees_kinematic_VMF1_IONEX/ppp_0.tree'\n",
    "        default_tree = '/apps/gipsyx/beta/GipsyX-Beta/share/gd2e/DefaultTreeSeries/PPP/ppp_0.tree'\n",
    "        input_tree = treeUtils.tree(default_tree)\n",
    "        ionex_files = pd.Series(\n",
    "            sorted(glob.glob(self.tmp_dir+'/IONEX_merged/' + self.ionex_type + '*')))\n",
    "        ionex_basenames = ionex_files.str.split('/', expand=True).iloc[:, -1]\n",
    "\n",
    "        out_df['year'] = ionex_basenames.str.slice(3, 7)\n",
    "        out_df['tree_path'] = self.tmp_dir + '/Trees/' + \\\n",
    "            ionex_basenames + '/'  # where to save tree file\n",
    "\n",
    "        for i in range(len(ionex_files)):\n",
    "            if not os.path.exists(out_df['tree_path'].iloc[i]):\n",
    "                os.makedirs(out_df['tree_path'].iloc[i])\n",
    "            input_tree.entries['Global:Ion2nd:StecModel:IonexFile'] = treeUtils.treevalue(\n",
    "                ionex_files[i])\n",
    "            input_tree.entries.pop(\n",
    "                'GRN_STATION_CLK_WHITE:State:Pos:ConstantAdj', None)\n",
    "\n",
    "            for option in self.tree_options:\n",
    "                input_tree.entries[option[0]] = treeUtils.treevalue(\n",
    "                    option[1])  # write standard parameters\n",
    "\n",
    "            input_tree.save(out_df['tree_path'][i] + 'ppp_0.tree')\n",
    "        # return year type path_trees\n",
    "\n",
    "        return out_df.set_index(['year'])   \n",
    "    def _gd2e(self, gd2e_set):\n",
    "        \n",
    "        if not os.path.exists(gd2e_set['output']):os.makedirs(gd2e_set['output'])\n",
    "        process = subprocess.Popen(['gd2e.py',\n",
    "                                    '-drEditedFile', gd2e_set['filename'],\n",
    "                                    '-recList', gd2e_set['station'],\n",
    "                                    '-runType', 'PPP',\n",
    "                                    '-GNSSproducts', self.gnss_products_dir,\n",
    "                                    '-treeSequenceDir', gd2e_set['tree_path'],\n",
    "                                    '-tdpInput', gd2e_set['tdp'],\n",
    "                                    '-staDb', self.staDb_dir,\n",
    "                                    '-gdCov'], cwd=gd2e_set['output'],stdout=subprocess.PIPE)\n",
    "        out, err = process.communicate()\n",
    "        print(str(gd2e_set['year'])+'/'+gd2e_set['dayofyear'])\n",
    "        #read tdp file (smooth0_0.tdp)\n",
    "        tdp, tdp_header = self._get_tdps_pn(gd2e_set['output']+'/smooth0_0.tdp')\n",
    "        #read tree file\n",
    "        debug_tree_file = gd2e_set['output']+'/debug.tree'\n",
    "        debug_tree = pd.read_csv(debug_tree_file,sep='#',header=None,error_bad_lines=True)\n",
    "        \n",
    "        runAgain = 'gd2e.py -drEditedFile {0} -recList {1} -runType PPP -GNSSproducts {2} -treeSequenceDir {3} -tdpInput {4} -staDb {5} -gdCov'.format(\n",
    "            gd2e_set['filename'],gd2e_set['station'],self.gnss_products_dir, gd2e_set['tree_path'],gd2e_set['tdp'],self.staDb_dir)\n",
    "        \n",
    "        rtgx_log = pd.read_csv(gd2e_set['output']+'/rtgx_ppp_0.tree.log0_0',sep='\\n',header=None).values\n",
    "        rtgx_err = pd.read_csv(gd2e_set['output']+'/rtgx_ppp_0.tree.err0_0',sep='\\n',header=None).values\n",
    "        #Kill folder with all files after reading\n",
    "        rmtree(path=gd2e_set['output'])\n",
    "        #create directory to store npz extracted data\n",
    "        os.makedirs(gd2e_set['output'])\n",
    "        np.savez_compressed(file=gd2e_set['output']+'/gipsyx_out',\n",
    "                            tdp=tdp,\n",
    "                            tdp_header=tdp_header,\n",
    "                            debug_tree=debug_tree,\n",
    "                            runAgain=runAgain,\n",
    "                            rtgx_log=rtgx_log,\n",
    "                            rtgx_err=rtgx_err,\n",
    "                            out = np.asarray(out),\n",
    "                            err = np.asarray(err))\n",
    "        \n",
    "    def gd2e(self):\n",
    "        trees_df = self.gen_trees()\n",
    "        gd2e_table = np.ndarray((len(self.stations_list)),dtype=object)\n",
    "        tmp_get_merge_tables = self.get_merge_table()\n",
    "        #loading list of analysed stations from drinfo npz file\n",
    "        drinfo_file = np.load(file=self.tmp_dir+'/rnx_dr/drinfo.npz')\n",
    "        drinfo_stations_list = drinfo_file['stations_list']\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.stations_list)):\n",
    "            tmp = pd.DataFrame()\n",
    "                                \n",
    "            station_index_in_drinfo = np.where(drinfo_stations_list==self.stations_list[i])[0][0]\n",
    "            tmp_merge_table = tmp_get_merge_tables[station_index_in_drinfo]\n",
    "            \n",
    "            filename = pd.Series(tmp_merge_table[:,4])#<============== Here correct for real station name i in drinfo main table\n",
    "            filename[tmp_merge_table[:,0]==3] = filename[tmp_merge_table[:,0]==3].str.slice(start=None, stop=-6) + '_30h.dr.gz'\n",
    "                        \n",
    "            tmp['filename'] = filename\n",
    "            tmp['class'] = tmp_merge_table[:,0]\n",
    "            tmp['year'] = pd.Series(tmp_merge_table[:,1]).dt.year.astype(str)\n",
    "            tmp['dayofyear'] = pd.Series(tmp_merge_table[:,1]).dt.dayofyear.astype(str).str.zfill(3)\n",
    "            tmp = tmp.join(other=trees_df,on='year')\n",
    "            tmp['tdp'] = self.tmp_dir+'/tropNom/' + tmp['year'] + '/' + tmp['dayofyear'] + '/' + self.tropNom_type\n",
    "            tmp['output'] = self.tmp_dir+'/gd2e/'+self.project_name +'/'+self.stations_list[i]+'/'+tmp['year']+ '/' + tmp['dayofyear']\n",
    "\n",
    "\n",
    "            tmp['station'] = self.stations_list[i]\n",
    "            \n",
    "            tmp['year'] = pd.to_numeric(tmp['year'])\n",
    "            \n",
    "            tmp = tmp[tmp['year'].isin(self.years_list)&tmp['class']!=0] #cleaning unused years (loaded from npz!!!)\n",
    "            \n",
    "            #Check if files exist (from what left):\n",
    "            file_exists = np.zeros(tmp.shape[0],dtype=int)\n",
    "            for j in range(tmp.shape[0]):\n",
    "                file_exists[j] = os.path.isfile(tmp['output'].iloc[j]+'/gipsyx_out.npz')\n",
    "            tmp['file_exists']=file_exists\n",
    "\n",
    "            if tmp[tmp['file_exists']==0].shape[0] ==0:\n",
    "                gd2e_table[i] = None\n",
    "                print('Station', self.stations_list[i], 'is already processed')\n",
    "            else:\n",
    "                gd2e_table[i] = tmp[tmp['file_exists']==0].to_records()#converting to records in order for mp to work properly as it doesn't work with pandas Dataframe\n",
    "                num_cores = self.num_cores if len(gd2e_table[i]) > self.num_cores else len(gd2e_table[i])\n",
    "\n",
    "                print('\\nStaion',self.stations_list[i],'processing starts...')\n",
    "                print('Number of files to be processed:', len(gd2e_table[i]),\n",
    "                      '\\nAdjusted number of cores:', num_cores)\n",
    "                try:\n",
    "                    pool = Pool(processes=num_cores)\n",
    "                    pool.map_async(func=self._gd2e,iterable=gd2e_table[i])\n",
    "                finally:\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "                print('Staion',self.stations_list[i],'done')\n",
    "        return gd2e_table\n",
    "    \n",
    "    def _get_tdps_pn(self, file):\n",
    "        # A working prototype for fast read and extract of tdp data\n",
    "        df = pd.read_table(file, sep='\\s+', header=None,\n",
    "                               usecols=[0, 1, 2, 3, 4], names=['time','nomvalue', 'value', 'sigma', 'type'])\n",
    "        df = df.pivot_table(index='time', columns='type')\n",
    "        \n",
    "        # Create output through dictionary concat\n",
    "        extracted_data = pd.concat({\n",
    "                                    'sigma'  : df['sigma'].iloc[:,np.arange(-11,-1)],\n",
    "                                    'nomvalue': df['nomvalue'].iloc[:,np.arange(-11,-1)],\n",
    "                                    'value'   : df['value'].iloc[:,np.arange(-11,-1)]\n",
    "                                    },axis=1)\n",
    "        \n",
    "        tdp = np.column_stack((df.index.values,extracted_data.values))\n",
    "        tdp_header = extracted_data.columns.values\n",
    "        \n",
    "        \n",
    "        return tdp,tdp_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:57:02.526847Z",
     "start_time": "2018-10-08T06:57:02.494732Z"
    }
   },
   "outputs": [],
   "source": [
    "'''CAMB test dataset 2002-2006 processing for noise analysis etc'''\n",
    "stations_list=['CAMB',]\n",
    "years_list=[2002,2003,2004,2005,2006]\n",
    "camb_aux = rnx2dr(stations_list=stations_list,years_list=years_list, project_name='camb_2002-2006_kinematic',num_cores=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:57:08.380054Z",
     "start_time": "2018-10-08T06:57:08.331275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to convert. All available rnx files are already converted\n"
     ]
    }
   ],
   "source": [
    "camb_aux.rnx2dr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:49:03.015475Z",
     "start_time": "2018-10-08T06:49:02.993109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnx_in = gx_convert.rnx2dr_gen_paths(rnx_files=camb_aux.rnx_files,stations_list=camb_aux.stations_list,tmp_dir=camb_aux.tmp_dir)[0][:,0]\n",
    "if_exists_array = np.ndarray((len(rnx_in)),dtype=bool)\n",
    "for i in range(len(rnx_in)):\n",
    "    if_exists_array[i] = not os.path.exists(rnx_in[i])\n",
    "len(rnx_in[if_exists_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:46:35.581149Z",
     "start_time": "2018-10-08T06:46:35.576903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if_exists_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:15:29.227698Z",
     "start_time": "2018-10-08T06:15:28.873810Z"
    }
   },
   "outputs": [],
   "source": [
    "# stations_list=['CAMB','NEWL','BRAE','HERT','LOFT','LERW','WEAR','SHEE']\n",
    "# years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB','NEWL','BRAE','HERT','LOFT','LERW','WEAR','SHEE']\n",
    "# stations_list=['CAMB','NEWL']\n",
    "years_list=[2007,2008,2009,2010]\n",
    "# years_list=[2007,]\n",
    "rnx = rnx2dr(stations_list=stations_list,years_list=years_list, project_name='bigf_np',num_cores=20,)\n",
    "# rnx.analyse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:15:45.279547Z",
     "start_time": "2018-10-08T06:15:37.931726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAMB station conversion to binary...\n",
      "Number of files to process: 1126 | Adj. num_cores: 20 | Chunksize: 57 | Done!\n",
      "NEWL station conversion to binary...\n",
      "Number of files to process: 1310 | Adj. num_cores: 20 | Chunksize: 66 | Done!\n",
      "BRAE station conversion to binary...\n",
      "Number of files to process: 1441 | Adj. num_cores: 20 | Chunksize: 73 | Done!\n",
      "HERT station conversion to binary...\n",
      "Number of files to process: 1451 | Adj. num_cores: 20 | Chunksize: 73 | Done!\n",
      "LOFT station conversion to binary...\n",
      "Number of files to process: 1443 | Adj. num_cores: 20 | Chunksize: 73 | Done!\n",
      "LERW station conversion to binary...\n",
      "Number of files to process: 1087 | Adj. num_cores: 20 | Chunksize: 55 | Done!\n",
      "WEAR station conversion to binary...\n",
      "Number of files to process: 1440 | Adj. num_cores: 20 | Chunksize: 72 | Done!\n",
      "SHEE station conversion to binary...\n",
      "Number of files to process: 1157 | Adj. num_cores: 20 | Chunksize: 58 | Done!\n"
     ]
    }
   ],
   "source": [
    "rnx.rnx2dr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:07:00.169711Z",
     "start_time": "2018-10-08T06:07:00.158041Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T06:07:29.315203Z",
     "start_time": "2018-10-08T06:07:28.968312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/camb0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/camb0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/camb0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/082/camb0820.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/083/camb0830.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/084/camb0840.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/newl0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/newl0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/newl0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/363/newl3630.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/364/newl3640.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/365/newl3650.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/brae0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/brae0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/brae0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/363/brae3630.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/364/brae3640.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/365/brae3650.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/hert0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/hert0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/hert0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/363/hert3630.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/364/hert3640.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/365/hert3650.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/loft0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/loft0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/loft0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/363/loft3630.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/364/loft3640.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/365/loft3650.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/lerw0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/lerw0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/lerw0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/011/lerw0110.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/012/lerw0120.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/013/lerw0130.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/wear0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/wear0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/wear0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/363/wear3630.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/364/wear3640.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/365/wear3650.10d.Z'],\n",
       "      dtype='<U70'),\n",
       "       array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/shee0030.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/004/shee0040.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/005/shee0050.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/363/shee3630.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/364/shee3640.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/365/shee3650.10d.Z'],\n",
       "      dtype='<U70')], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnx._select_rnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T05:08:59.641675Z",
     "start_time": "2018-10-08T05:08:59.637753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2007, 2008, 2009, 2010]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnx.years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T05:09:11.753634Z",
     "start_time": "2018-10-08T05:09:11.749915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnx.rnx_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T05:12:04.975708Z",
     "start_time": "2018-10-08T05:12:04.640106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/001/camb0010.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/002/camb0020.07d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2007/003/camb0030.07d.Z',\n",
       "       ...,\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/082/camb0820.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/083/camb0830.10d.Z',\n",
       "       '/mnt/Data/bogdanm/GNSS_data/BIGF_data/daily30s/2010/084/camb0840.10d.Z'],\n",
       "      dtype='<U70')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gx_aux.select_rnx(rnx_dir=rnx.rnx_dir,stations_list=rnx.stations_list,years_list=rnx.years_list)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T03:46:58.886632Z",
     "start_time": "2018-10-08T03:46:58.872982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.aux' from '/home/bogdanm/Desktop/GipsyX_Wrapper/lib/aux.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-08T03:44:58.964969Z",
     "start_time": "2018-10-08T03:44:58.952337Z"
    }
   },
   "outputs": [],
   "source": [
    "%aimport lib.aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T13:17:37.903997Z",
     "start_time": "2018-09-25T13:17:37.754171Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx._get_tdps_pn(file='/mnt/Data/bogdanm/tmp_GipsyX/gd2e/2007/CAMB/001/smooth0_0.tdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T05:09:34.797408Z",
     "start_time": "2018-09-25T05:09:34.706589Z"
    }
   },
   "outputs": [],
   "source": [
    "_get_tdps_pn(file='/mnt/Data/bogdanm/tmp_GipsyX/gd2e/2007/CAMB/001/smooth0_0.tdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T06:51:12.681604Z",
     "start_time": "2018-09-24T06:50:06.288839Z"
    }
   },
   "outputs": [],
   "source": [
    "# rnx.gen_tropnom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T12:54:39.084801Z",
     "start_time": "2018-09-25T12:54:39.062549Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_tdps_pn(file):\n",
    "    # A working prototype for fast read and extract of tdp data\n",
    "        df = pd.read_table(file, sep='\\s+', header=None,\n",
    "                               usecols=[0, 1, 2, 3, 4], names=['time','nomvalue', 'value', 'sigma', 'type'])\n",
    "        df = df.pivot_table(index='time', columns='type')\n",
    "        \n",
    "        # Create output through dictionary concat\n",
    "        extracted_data = pd.concat({\n",
    "                                    'sigma '  : df['sigma'].iloc[:,np.arange(-11,-1)],\n",
    "                                    'nomvalue': df['nomvalue'].iloc[:,np.arange(-11,-1)],\n",
    "                                    'value'   : df['value'].iloc[:,np.arange(-11,-1)]\n",
    "                                    },axis=1)\n",
    "        \n",
    "        tdp = np.column_stack((df.index.values,extracted_data.values))\n",
    "        tdp_header = extracted_data.columns.values\n",
    "        \n",
    "        \n",
    "        return tdp,tdp_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T12:55:08.486582Z",
     "start_time": "2018-09-25T12:55:08.383774Z"
    }
   },
   "outputs": [],
   "source": [
    "tdps = _get_tdps_pn(file='/mnt/Data/bogdanm/tmp_GipsyX/gd2e/2007/CAMB/001/smooth0_0.tdp')\n",
    "tdps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-01T11:06:48.288671Z",
     "start_time": "2018-10-01T10:49:34.586706Z"
    }
   },
   "outputs": [],
   "source": [
    "'''FINAL WORKING VERSION. Processing with tropNominal tdp. no synth'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]T\n",
    "rnx = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_tdp_in',\n",
    "                   num_cores=25,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp')\n",
    "rnx.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T03:38:53.634587Z",
     "start_time": "2018-09-26T03:38:52.345024Z"
    }
   },
   "outputs": [],
   "source": [
    "'''FINAL WORKING VERSION. Processing small dataset with synthetic waveform in tropNominal tdp'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_synth_penna',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_penna')\n",
    "rnx_synth.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-26T04:12:58.262817Z",
     "start_time": "2018-09-26T03:58:46.124700Z"
    }
   },
   "outputs": [],
   "source": [
    "'''All tropnom xyz are 6 mm'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_synth_penna_6',\n",
    "                   num_cores=20,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_penna_6')\n",
    "rnx_synth.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T14:38:04.402800Z",
     "start_time": "2018-09-25T13:58:24.798896Z"
    }
   },
   "outputs": [],
   "source": [
    "'''STADB 0 FINAL WORKING VERSION. Processing small dataset with synthetic waveform in tropNominal tdp'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_staDb_0',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_0_staDb_0')\n",
    "rnx_synth.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-25T15:17:51.816497Z",
     "start_time": "2018-09-25T14:38:04.404572Z"
    }
   },
   "outputs": [],
   "source": [
    "'''STADB 1 FINAL WORKING VERSION. Processing small dataset with synthetic waveform in tropNominal tdp'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_staDb_1',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_0_staDb_1')\n",
    "rnx_synth.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:16:15.564093Z",
     "start_time": "2018-09-22T11:16:15.532364Z"
    }
   },
   "outputs": [],
   "source": [
    "'''TEST TEST TEST'''\n",
    "years_list=[2007,]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_synth_test',\n",
    "                   num_cores=20,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_synth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T11:16:26.144056Z",
     "start_time": "2018-09-22T11:16:23.045574Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_synth.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T04:42:46.280945Z",
     "start_time": "2018-09-11T04:42:46.244800Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset with synthetic waveform in tropNominal tdp'''\n",
    "years_list=[2007,]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth_penna = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_synth_penna',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_synth_penna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T04:55:07.438712Z",
     "start_time": "2018-09-11T04:42:47.866151Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_synth_penna.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T05:08:56.508046Z",
     "start_time": "2018-09-18T05:08:56.489630Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset with synthetic waveform in tropNominal tdp (penna) and tids off'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_synth_penna_tides_off = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_synth_penna_tides_off',\n",
    "                   num_cores=20,\n",
    "                   tree_options = tree_options_tides_off,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_staDb_synth_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T05:28:46.428718Z",
     "start_time": "2018-09-18T05:08:58.039518Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_synth_penna_tides_off.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T05:28:46.463386Z",
     "start_time": "2018-09-18T05:28:46.431639Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset with synthetic waveform in tropNominal tdp (penna) and tides off'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_staDb_tides_off = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_staDb_tides_off',\n",
    "                   num_cores=20, \n",
    "                    tree_options = tree_options_tides_off,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_0_staDb_s1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T05:48:21.849334Z",
     "start_time": "2018-09-18T05:28:46.467516Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_staDb_tides_off.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T05:06:21.977146Z",
     "start_time": "2018-09-17T05:06:21.967037Z"
    }
   },
   "outputs": [],
   "source": [
    "# rnx_synth_penna_ambres_off.gen_trees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T05:08:57.074169Z",
     "start_time": "2018-09-17T05:08:57.060523Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset VMF1 tropNominal tdp (no synth) and ambres off'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_ambres_off = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_ambres_off',\n",
    "                   num_cores=20, \n",
    "                    tree_options = tree_options_ambres_off,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T05:24:23.141766Z",
     "start_time": "2018-09-17T05:09:00.914786Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_ambres_off.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T02:28:34.541847Z",
     "start_time": "2018-09-12T02:28:34.485970Z"
    }
   },
   "outputs": [],
   "source": [
    "# '''Processing small dataset with synthetic waveform in tropNominal tdp/ penna diff'''\n",
    "# years_list=[2007,2008,2009,2010,2011,2012]\n",
    "# stations_list=['CAMB',]\n",
    "# rnx_synth_penna_diff = rnx2dr(stations_list=stations_list,\n",
    "#                    years_list=years_list,\n",
    "#                    project_name='camb_synth_penna_diff',\n",
    "#                    num_cores=10,\n",
    "#                    tropNom_type='30h_tropNominalOut_VMF1.tdp_synth_penna_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T03:10:12.755892Z",
     "start_time": "2018-09-12T02:29:53.764249Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_synth_penna_diff.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T08:56:06.339192Z",
     "start_time": "2018-09-12T08:56:06.323341Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset with synthetic waveform in tropNominal tdp'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_camb = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_tdp_corrected',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T09:33:43.157440Z",
     "start_time": "2018-09-12T08:56:08.220007Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_camb.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T04:23:46.957750Z",
     "start_time": "2018-09-13T04:23:46.952176Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset with synthetic waveform in tropNominal tdp. Nominal values = 0 | StaDb values |  '''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_camb_0_staDb_0 = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_tdp_corrected_0_staDb_0',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_0_staDb_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-13T04:23:46.555Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_camb_0_staDb_0.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T09:04:18.993056Z",
     "start_time": "2018-09-13T09:04:18.970019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final processing of test dataset with tdp files of: 0 | Synth | 1\n",
    "# '30h_tropNominalOut_VMF1.tdp_0_synth_1'\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_camb_0_synth_1 = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_tdp_corrected_0_synth_1',\n",
    "                   num_cores=10,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_0_synth_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T09:41:45.589351Z",
     "start_time": "2018-09-13T09:04:28.918094Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_camb_0_synth_1.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:16:36.246250Z",
     "start_time": "2018-09-14T04:16:36.240378Z"
    }
   },
   "outputs": [],
   "source": [
    "'''PENNA HERE'''\n",
    "# Final processing of test dataset with tdp files of: 0 | Synth | 1\n",
    "# '30h_tropNominalOut_VMF1.tdp_0_synth_1'\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_camb_staDb_synth_1 = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_tdp_corrected_staDb_synth_1',\n",
    "                   num_cores=12,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_staDb_synth_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:49:17.643670Z",
     "start_time": "2018-09-14T04:16:37.962384Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_camb_staDb_synth_1.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T04:15:59.896860Z",
     "start_time": "2018-09-14T04:15:59.890750Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Processing small dataset with synthetic waveform in tropNominal tdp'''\n",
    "years_list=[2007,2008,2009,2010,2011,2012]\n",
    "stations_list=['CAMB',]\n",
    "rnx_camb_0_staDb_s1 = rnx2dr(stations_list=stations_list,\n",
    "                   years_list=years_list,\n",
    "                   project_name='camb_tdp_corrected_0_staDb_s1',\n",
    "                   num_cores=16,\n",
    "                   tropNom_type='30h_tropNominalOut_VMF1.tdp_0_staDb_s1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T06:44:48.797352Z",
     "start_time": "2018-09-13T06:20:00.039225Z"
    }
   },
   "outputs": [],
   "source": [
    "rnx_camb_0_staDb_s1.gd2e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:06:54.932889Z",
     "start_time": "2018-08-23T16:06:54.843491Z"
    }
   },
   "outputs": [],
   "source": [
    "synth = rnx_synth._get_tdps_pn(file='/home/bogdanm/Desktop/CAMB_synthtdp_test/smooth0_0.tdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:06:55.051424Z",
     "start_time": "2018-08-23T16:06:54.967129Z"
    }
   },
   "outputs": [],
   "source": [
    "proper = rnx_synth._get_tdps_pn(file='/home/bogdanm/Desktop/CAMB_test/smooth0_0.tdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:06:55.354487Z",
     "start_time": "2018-08-23T16:06:55.350110Z"
    }
   },
   "outputs": [],
   "source": [
    "synth_df = pd.DataFrame(synth[0][:,1:],index=synth[0][:,0],columns=synth[1])\n",
    "proper_df = pd.DataFrame(proper[0][:,1:],index=proper[0][:,0],columns=proper[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-23T16:07:52.101867Z",
     "start_time": "2018-08-23T16:07:52.092269Z"
    }
   },
   "outputs": [],
   "source": [
    "synth_df.iloc[0,2]\n",
    "#4878480.428133955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T13:15:24.949031Z",
     "start_time": "2018-09-22T13:15:24.921632Z"
    }
   },
   "outputs": [],
   "source": [
    "np.load('/mnt/Data/bogdanm/tmp_GipsyX/gd2e/camb_staDb/CAMB/2007/001/gipsyx_out.npz')['runAgain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T09:56:52.705064Z",
     "start_time": "2018-09-14T09:56:52.700932Z"
    }
   },
   "outputs": [],
   "source": [
    "J2000_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
